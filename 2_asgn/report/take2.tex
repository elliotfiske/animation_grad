\documentclass[report,12pt,notitlepage,onecolumn]{article}
% define the title
\author{Elliot Fiske}
\title{Optimizing Linear Algebra Performance for Programmers}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage[pdftex,colorlinks,%
citecolor=cyan,%
filecolor=cyan,%
linkcolor=blue,%
urlcolor=cyan,%
]{hyperref}
\begin{document}

% generates the title
\maketitle
% insert the table of contents
%\tableofcontents

\newcommand{\matr}[1]{\mathbf{#1}} % undergraduate algebra version

\section{Multiplying Many Matrices and One Vector}
Consider the following matrix operation:
\begin{equation}
Result = \underbrace{\matr{A} * \matr{B} * \matr{C} * \matr{D} * \matr{E}}
_\text{100 by 100}{} * \underbrace{\vec{x}}
_\text{100 by 1}
\end{equation}

If this operation was done programmatically, with a compiler that evaluated each operation left-to-right, there would be over 4 million multiplication operations: \begin{equation}\label{eq1}
\underbrace{100 * 100 * 100 * \textbf{4}}_\text{\parbox{3cm}{4 instances of multiplying a 100x100 matrix by a 100x100 matrix}}{} + \underbrace{100 * 100}_\text{\parbox{2cm}{1 instance of multiplying a 100x100 matrix by a 100x1 vector}} = 4,010,000
\end{equation}

However, if we instead first evaluate $\matr{E}{} * \vec{x}$, then evaluate $\matr{D}{} * (\matr{E}{} * \vec{x})$, etc. there will be just over 1 million multiplication operations: 
\begin{equation}\label{eq2}
\underbrace{100 * 100 * \textbf{4}}_\text{\parbox{3cm}{4 instances of multiplying a 100x100 matrix by a 100x1 vector}}{} + \underbrace{100 * 100 * 100}_\text{\parbox{2cm}{1 instance of multiplying a 100x100 matrix by a 100x100 matrix}} = 1,040,000
\end{equation}

I used Eigen and C++ to practically test this, timing how long it took to do the approach from Equation~\ref{eq1} vs. the approach from Equation~\ref{eq2} with various matrix sizes.

\begin{table}[!h]
	\begin{tabular}{| l | l | l | l |}
		\hline
		Matrix Dimension & Slow Method Time ($\mu$s) & Fast Method Time ($\mu$s) \\ \hline
		10 & 162 & 52 \\ \hline
		50 & 13,291 & 738  \\ \hline
		100 & 32,515 & 2561  \\ \hline
		300 & 537,397 & 17,399  \\ \hline
		1000 & 19,730,814 & 173,000  \\
		\hline
	\end{tabular}
	\caption{Timing the dumb way vs. the smart way\label{data1}}
\end{table}

For each dimension, the 'smart' method is orders of magnitude faster, becoming better as the dimension increases.

\section{Calculating the Inverse of a Matrix}

Consider the following matrix operation:
\begin{equation}\label{inverse_eqn}
Result = \vec{x}^{T} * \matr{A}^{T} * \matr{B}^{-1} * \matr{A} * \vec{x}
\end{equation}

One approach is to simply calculate the inverse of $\matr{B}$ then multiply it with the rest of the terms. However, calculating the inverse of a matrix is a very expensive operation. Instead, consider rearranging the equation like so:
\begin{equation}
\begin{split}\label{inverse_eqn_better}
\matr{B}b &=  \matr{A} * \vec{x} \\
Result &= \vec{x}^{T} * \matr{A}^{T} * b
\end{split}
\end{equation}

It is a much cheaper operation to solve a $\matr{A}\vec{x} = \matr{B}$ matrix problem than it is to find an inverse of a matrix. I used Eigen and C++ to practically test this, timing how long it takes to do the approach from Equation~\ref{inverse_eqn} vs. the approach from Equation~\ref{inverse_eqn_better}.

\begin{table}[!h]
	\begin{tabular}{| l | l | l | l |}
		\hline
		Matrix Dimension & $inverse()$ Time ($\mu$s) & $solve()$ Time ($\mu$s) \\ \hline
		10 & 61 & 70 \\ \hline
		50 & 278 & 267  \\ \hline
		100 & 670 & 935  \\ \hline
		300 & 9599 & 17,104  \\ \hline
		1000 & 320,196 & 244,522 \\
		\hline
	\end{tabular}
	\caption{Using $inverse()$ vs. using $solve()$\label{data_inverse}}
\end{table}

Below a 50x50 matrix, the two methods are fairly equal in running time. However, at dimension 100 and above the solve() method is cleary superior.

\end{document}